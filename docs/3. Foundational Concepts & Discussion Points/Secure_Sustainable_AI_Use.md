## 🔒 1:15–1:30 — Secure & Sustainable AI Use

### 🌟 Purpose

Ground the seminar in responsible use. Provide participants with confidence about what they *can* do now without risking sensitive data, and how to think long-term about integrating AI in secure, governed ways.

---

### 📄 Framing Statement

> “Even if the AI gets the right answer, if you use the wrong data or tool, it can create serious risk. Let’s talk about what’s safe, what’s smart, and what’s sustainable in your setting.”

---

## 🔍 Key Principles

### Core AI Implementation Principles

1. **People-first Design**
   AI should enhance human capabilities, not replace human judgment. TSOs remain central to teaching support delivery.

2. **Responsible and Ethical Use of AI**
   Consider the impact on students, staff, and academic integrity. Ensure transparency in AI-assisted processes.

3. **Secure and Responsible Design**
   Align with Curtin's IT security policies and TEQSA compliance requirements. Protect student data and privacy.

### Practical Safety Guidelines

1. **Use mock or public data in all experiments**
   Especially in exploratory or non-integrated tools (e.g., ChatGPT, Claude). Never use real student data outside secure systems.

2. **Assume anything entered into a public AI chat is public**
   Unless you *know* the tool offers enterprise privacy, don't paste student-identifiable or sensitive university information.

3. **Use your secure platforms for sensitive work**
   Copilot inside Microsoft 365 is the ONLY approved tool for university data. Your data stays in your tenant and isn't used to train models.

4. **Prototype in chat tools, deploy in secure environments**
   Test prompts with mock data in ChatGPT, then migrate your workflow to secure Copilot or approved university systems.

5. **Governance is a team sport**
   Involve your IT/security leads and Teaching & Learning committee when formalising AI use. Keep logs, version prompts, and review how tools access data.

---

# 🤖 AI Tools: What’s Under the Hood?

## 1. What is Copilot / ChatGPT?
- LLMs trained to predict useful text.
- Copilot runs inside M365 tools.
- ChatGPT is public-facing unless restricted.

## 2. What is Prompt Engineering?
- Structure your input to guide output.
- CRAFT = Context, Role, Action, Format, Task.

## 3. What Are Agents?
- Tools that reason + act using steps (e.g. AutoGPT).
- Not used directly in Copilot, but conceptually similar.

## 4. Key Concepts
- Context Window: How much AI can consider at once.
- Temperature: Randomness/creativity setting.
- Not “thinking” — just predicting.

## 5. Security & Risk
- Copilot: Internal to your M365 tenant.
- ChatGPT: Avoid client-sensitive data unless secured.
- Always verify AI output.

## 6. What are MCPs?
- “Model Context Protocol”
- An open protocol that lets AI assistants securely connect to external data sources and tools.




---

## 🚧 Platform Governance Comparison

| Tool             | Use Case                     | Governance Notes                                                    |
| ---------------- | ---------------------------- | ------------------------------------------------------------------- |
| **Copilot (M365)**| Primary tool for university work | ✅ APPROVED for internal docs and student data. Educational license. |
| **ChatGPT**      | Personal exploration only    | ❌ NOT for university data. Personal learning with mock data only.   |
| **Claude**       | Personal exploration only    | ❌ NOT for university data. Personal learning with mock data only.   |
| **Gemini**       | Personal exploration only    | ❌ NOT for university data. Personal learning with mock data only.   |
| **Libre Chat**   | Future possibility           | 🔄 Under review by IT (Brenton/Matt). Not yet approved.            |

---

## 🚀 Strategy Summary: Prompt, Process, Protect

> 1. **Prompt It** — Use AI chat to test your thinking, summarise ideas, and shape workflows.
> 2. **Process It** — Move successful chains into tools like Copilot, n8n, or Zapier.
> 3. **Protect It** — When it’s sensitive or client-facing, work within secure systems.

---

## 🚶 What to Say if Asked: "Can we use this for real student data?"

> "Yes, but only in tools governed by Curtin's enterprise environment — like Copilot in Microsoft 365. You can prototype freely in public AI with mock data, but never use real student data outside secure university systems."

---

## 🏛 Key Takeaway

> "AI is only as smart as your context — and only as safe as your systems. Microsoft Copilot is our approved tool for university work. Other AI tools are for personal exploration only."

