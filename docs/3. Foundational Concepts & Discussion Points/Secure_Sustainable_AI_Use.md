## ğŸ”’ 1:15â€“1:30 â€” Secure & Sustainable AI Use

### ğŸŒŸ Purpose

Ground the seminar in responsible use. Provide participants with confidence about what they *can* do now without risking sensitive data, and how to think long-term about integrating AI in secure, governed ways.

---

### ğŸ“„ Framing Statement

> â€œEven if the AI gets the right answer, if you use the wrong data or tool, it can create serious risk. Letâ€™s talk about whatâ€™s safe, whatâ€™s smart, and whatâ€™s sustainable in your setting.â€

---

## ğŸ” Key Principles

### Core AI Implementation Principles

1. **People-first Design**
   AI should enhance human capabilities, not replace human judgment. TSOs remain central to teaching support delivery.

2. **Responsible and Ethical Use of AI**
   Consider the impact on students, staff, and academic integrity. Ensure transparency in AI-assisted processes.

3. **Secure and Responsible Design**
   Align with Curtin's IT security policies and TEQSA compliance requirements. Protect student data and privacy.

### Practical Safety Guidelines

1. **Use mock or public data in all experiments**
   Especially in exploratory or non-integrated tools (e.g., ChatGPT, Claude). Never use real student data outside secure systems.

2. **Assume anything entered into a public AI chat is public**
   Unless you *know* the tool offers enterprise privacy, don't paste student-identifiable or sensitive university information.

3. **Use your secure platforms for sensitive work**
   Copilot inside Microsoft 365 is enterprise-grade: your data stays in your tenant and isn't used to train models.

4. **Prototype in chat tools, deploy in secure environments**
   Test prompts with mock data in ChatGPT, then migrate your workflow to secure Copilot or approved university systems.

5. **Governance is a team sport**
   Involve your IT/security leads and Teaching & Learning committee when formalising AI use. Keep logs, version prompts, and review how tools access data.

---

# ğŸ¤– AI Tools: Whatâ€™s Under the Hood?

## 1. What is Copilot / ChatGPT?
- LLMs trained to predict useful text.
- Copilot runs inside M365 tools.
- ChatGPT is public-facing unless restricted.

## 2. What is Prompt Engineering?
- Structure your input to guide output.
- CRAFT = Context, Role, Action, Format, Task.

## 3. What Are Agents?
- Tools that reason + act using steps (e.g. AutoGPT).
- Not used directly in Copilot, but conceptually similar.

## 4. Key Concepts
- Context Window: How much AI can consider at once.
- Temperature: Randomness/creativity setting.
- Not â€œthinkingâ€ â€” just predicting.

## 5. Security & Risk
- Copilot: Internal to your M365 tenant.
- ChatGPT: Avoid client-sensitive data unless secured.
- Always verify AI output.

## 6. What are MCPs?
- â€œModel Context Protocolâ€
- An open protocol that lets AI assistants securely connect to external data sources and tools.




---

## ğŸš§ Platform Governance Comparison

| Tool             | Use Case                     | Governance Notes                                                    |
| ---------------- | ---------------------------- | ------------------------------------------------------------------- |
| **ChatGPT**      | Rapid prototyping, summaries | âŒ No client data. Treat like a public whiteboard.                   |
| **Copilot Chat** | Secure drafting in M365      | âœ… Safe for internal docs. Respects your permissions.                |
| **Gemini**       | Research, document analysis  | âš ï¸ Check what is shared via APIs. Not built for confidential input. |
| **Claude**       | Long context reasoning       | âŒ Same privacy limits as ChatGPT. Use mock data only.               |

---

## ğŸš€ Strategy Summary: Prompt, Process, Protect

> 1. **Prompt It** â€” Use AI chat to test your thinking, summarise ideas, and shape workflows.
> 2. **Process It** â€” Move successful chains into tools like Copilot, n8n, or Zapier.
> 3. **Protect It** â€” When itâ€™s sensitive or client-facing, work within secure systems.

---

## ğŸš¶ What to Say if Asked: "Can we use this for real student data?"

> "Yes, but only in tools governed by Curtin's enterprise environment â€” like Copilot in Microsoft 365. You can prototype freely in public AI with mock data, but never use real student data outside secure university systems."

---

## ğŸ› Key Takeaway

> â€œAI is only as smart as your context â€” and only as safe as your systems. Work with it like a colleague: trust it, but verify, and never put it in a position to break the rules.â€

